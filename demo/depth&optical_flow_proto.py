import os
import glob
import torch
import cv2
import numpy as np
import random
import sys
import torch.nn.functional as F
from PIL import Image
import importlib
from torchvision import transforms
from os.path import join
from segment_anything import SamAutomaticMaskGenerator, sam_model_registry, SamPredictor
import time
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

from transformers import pipeline
import matplotlib.pyplot as plt

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.image import flip_tensor
AOT_PATH = os.path.join(os.path.dirname(__file__), '..')
import dataloaders.video_transforms as tr
from networks.engines import build_engine
from utils.checkpoint import load_network
from networks.models import build_vos_model
from utils.metric import pytorch_iou
from pathlib import Path
from collections import deque  # ëìŠ¤ ë³€í™”ëŸ‰ ì €ì¥ì„ ìœ„í•œ ì¶”ê°€
from scipy.stats import norm

base_path = os.path.dirname(os.path.abspath(__file__))
demo_video = 'p_09'
img_files = sorted(glob.glob(join(base_path, demo_video, '*.jp*'))) 
point_box_prompts=[]

# ëœë¤ ì‹œë“œ ì„¤ì •
def seed_torch(seed=0):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)

seed_torch(1000000007)
torch.set_num_threads(4)
torch.autograd.set_grad_enabled(False)

cur_colors = [(0, 255, 255), # yellow b g r
              (255, 0, 0), # blue
              (0, 255, 0), # green
              (0, 0, 255), # red
              (255, 255, 255), # white
              (0, 0, 0), # black
              (255, 255, 0), # Cyan
              (225, 228, 255), # MistyRose
              (180, 105, 255), # HotPink
              (255, 0, 255), # Magenta
              ]*100

# ëìŠ¤ ë³€í™”ëŸ‰ ì €ì¥ êµ¬ì¡°  -> ë™ì  íë¡œ ë³€í™˜ê°€ëŠ¥? ê³ ì •ëœ í¬ê¸°ë³´ë‹¤ íì˜ í¬ê¸°ë„ ë™ì ìœ¼ë¡œ ë°”ë€Œì—ˆìœ¼ë©´ ì¢‹ê² ìŒìŒ
depth_history = {}  # {object_id: deque([d1, d2, ..., dN])}
depth_diff_history = []
N = 10  # ìµœê·¼ í”„ë ˆì„ ê°œìˆ˜
adaptive_threshold = None

class DepthThresholdManager:
    def __init__(self, min_N=5, max_N=20, base_N=10, threshold_std=0.05, removal_threshold=0.02, max_frame_diff=10):
        self.depth_history = {}  # {object_id: deque([(frame_idx, depth), ...])}
        self.min_N = min_N  # ìµœì†Œ ì €ì¥ í”„ë ˆì„ ê°œìˆ˜
        self.max_N = max_N  # ìµœëŒ€ ì €ì¥ í”„ë ˆì„ ê°œìˆ˜
        self.base_N = base_N  # ê¸°ë³¸ í”„ë ˆì„ ê°œìˆ˜
        self.threshold_std = threshold_std  # ë³€ë™ì„± ê¸°ì¤€
        self.removal_threshold = removal_threshold  # ì œê±° ê¸°ì¤€ (ë³€í™”ëŸ‰ì´ ë„ˆë¬´ ì‘ì„ ë•Œ)  -> depth_diff ê°™ì€ ê°’ìœ¼ë¡œ ë°”ê¾¸ëŠ” ë°©ë²• ìƒê°í•˜ê¸°
        self.max_frame_diff = max_frame_diff  # ì˜¤ë˜ëœ í”„ë ˆì„ ì‚­ì œ ê¸°ì¤€

    def update_depth(self, obj_id, depth_value, frame_idx):
        """ê°ì²´ë³„ ëìŠ¤ ë³€í™”ëŸ‰ ì €ì¥ (ë™ì  ê¸¸ì´ ì¡°ì • + ì˜¤ë˜ëœ í”„ë ˆì„ ì‚­ì œ)"""
        if isinstance(obj_id, deque):
            obj_id = obj_id[0][0] 
    
        if obj_id not in self.depth_history:
            self.depth_history[obj_id] = deque(maxlen=self.base_N)

        history = self.depth_history[obj_id]
         # ğŸ“Œ (1) ì´ˆê¸° deque ìƒíƒœ ì¶œë ¥
        #print(f"[DEBUG] Initial history for obj {obj_id}: {list(history)}")

        # ìƒˆë¡œìš´ ê°’ ì¶”ê°€
        frame_idx = int(frame_idx) if isinstance(frame_idx, (int, float)) else 0  # ì˜ˆì™¸ ì²˜ë¦¬
        history.append((frame_idx, depth_value))
        
        # ğŸ“Œ (2) ê°’ ì¶”ê°€ í›„ deque ìƒíƒœ ì¶œë ¥
        #print(f"[DEBUG] After append for obj {obj_id}: {list(history)}")

        if len(history) < 3:
            return history  # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ìœ ì§€
    
        # í˜„ì¬ ëìŠ¤ê°’ê³¼ì˜ ì°¨ì´ ê³„ì‚°
        depth_values = np.array([depth for _, depth in history])
        differences = np.abs(depth_values - depth_value)

        # (1) ë³€ë™ì„± ê¸°ë°˜ ê¸¸ì´ ì¡°ì •
        std_depth = np.std(np.diff(depth_values)) if len(depth_values) > 1 else 0
        if std_depth > self.threshold_std:
            new_N = min(self.max_N, len(history) + 2)  # ë³€ë™ì„±ì´ í¬ë©´ ì¦ê°€
        else:
            new_N = max(self.min_N, len(history) - 2)  # ë³€ë™ì„±ì´ ì‘ìœ¼ë©´ ê°ì†Œ
    
        self.depth_history[obj_id] = deque(history, maxlen=new_N)  # âœ… ê¸¸ì´ ì—…ë°ì´íŠ¸ ë°˜ì˜
        
        # ğŸ“Œ (3) ê¸¸ì´ ì¡°ì • í›„ deque ìƒíƒœ ì¶œë ¥
        #print(f"[DEBUG] After resizing for obj {obj_id} (new maxlen={new_N}): {list(self.depth_history[obj_id])}")

        # (2) ë³€í™”ëŸ‰ì´ ë„ˆë¬´ ì‘ì€ ë°ì´í„° ì‚­ì œ 
        if len(history) > self.min_N:
            idx_to_remove = np.argmin(differences)  # í˜„ì¬ê°’ê³¼ ê°€ì¥ ì°¨ì´ê°€ ì ì€ ê°’ ì„ íƒ
            history_list = list(history)
            history_list.pop(idx_to_remove)  # ë¦¬ìŠ¤íŠ¸ ë³€í™˜ í›„ ì‚­ì œ
            self.depth_history[obj_id] = deque(history_list, maxlen=new_N)  # ë‹¤ì‹œ deque ë³€í™˜
            print(f"[REMOVE] Low variance detected, removing index {idx_to_remove} for object {obj_id}")
            
             # ğŸ“Œ (4) ê°’ ì œê±° í›„ deque ìƒíƒœ ì¶œë ¥
            print(f"[DEBUG] After removing low variance data for obj {obj_id}: {list(self.depth_history[obj_id])}")

        # (3) ì˜¤ë˜ëœ í”„ë ˆì„ ì‚­ì œ (frame_idx ê¸°ì¤€ í•„í„°ë§ + ìµœì‹ ìˆœ ì •ë ¬)
        filtered_history = [(f, d) for f, d in self.depth_history[obj_id] if frame_idx - f <= self.max_frame_diff]
        filtered_history.sort(reverse=True, key=lambda x: x[0])  # ìµœì‹  í”„ë ˆì„ ìš°ì„  ì •ë ¬
        self.depth_history[obj_id] = deque(filtered_history[:new_N], maxlen=new_N)  # âœ… ê¸¸ì´ ìœ ì§€
        
         # ğŸ“Œ (5) ìµœì¢… deque ìƒíƒœ ì¶œë ¥
        #print(f"[DEBUG] Final history for obj {obj_id}: {list(self.depth_history[obj_id])}")

        return self.depth_history[obj_id]
        
manager = DepthThresholdManager() # í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±(Queuing)

############################################################################################################## Thresholding  ##########################################################################

class AdaptiveThreshold:
    def __init__(self):
        """ë°ì´í„° ì ì‘í˜• ì‹ ë¢° êµ¬ê°„ + ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸ ê¸°ë°˜ ìŠ¤ë ˆì‰¬í™€ë“œ"""
        self.mean = None
        self.std = None
    
    def compute_confidence_interval_threshold(self, depth_diff_history):
        """ê³ ë„í™”ëœ ì ì‘í˜• ì‹ ë¢° êµ¬ê°„ ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚°"""
        confidence = compute_dynamic_confidence(depth_diff_history)
    
        depth_diff_history = np.array(depth_diff_history)
        depth_diff_history = depth_diff_history[~np.isnan(depth_diff_history)]  # NaN ì œê±°

        if len(depth_diff_history) < 5:
            return np.mean(depth_diff_history) + np.std(depth_diff_history) * 0.1  # ê¸°ë³¸ê°’ ë°˜í™˜

        # ì´ìƒì¹˜ ì œê±° (IQR ë°©ë²•)
        q1, q3 = np.percentile(depth_diff_history, [25, 75])
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        filtered_depth_diff = depth_diff_history[
        (depth_diff_history >= lower_bound) & (depth_diff_history <= upper_bound)
    ]

        if len(filtered_depth_diff) < 5:
            filtered_depth_diff = depth_diff_history  # ì´ìƒì¹˜ ì œê±° í›„ ìƒ˜í”Œ ë¶€ì¡±í•˜ë©´ ì›ë³¸ ìœ ì§€

        mean_diff = np.mean(filtered_depth_diff)
        std_diff = np.std(filtered_depth_diff)

        # ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê· (EWMA) ì ìš©
        alpha = 0.2
        ewma_mean = filtered_depth_diff[0]
        for val in filtered_depth_diff[1:]:
            ewma_mean = alpha * val + (1 - alpha) * ewma_mean

        # ì‹ ë¢°ìˆ˜ì¤€ì— ë”°ë¥¸ Z-score ê³„ì‚°
        z_score = norm.ppf(1 - (1 - confidence) / 2)

        # ë™ì  ìŠ¤ì¼€ì¼ë§ íŒ©í„° ì ìš©
        scaling_factor = min(1.0, 0.1 + std_diff / np.max(filtered_depth_diff))

        return ewma_mean + z_score * std_diff * scaling_factor    
    
    def update_bayesian(self, new_value, depth_diff_history):
        """ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸ ë°©ì‹ìœ¼ë¡œ ì ì‘í˜• a ì ìš©"""
        alpha = compute_dynamic_alpha(depth_diff_history)  # Î± ê°’ ìë™ ì¡°ì •

        if np.isnan(new_value):
            return self.mean if self.mean is not None else 5.0 # ê¸°ì¡´ í‰ê·  ìœ ì§€
        
        #ì´ˆê¸°ê°’ ì„¤ì •
        if self.mean is None:
            self.mean = new_value
            self.std = 1e-6 # ì‘ì€ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì•ˆì •ì„± í™•ë³´ë³´
        else:
            # ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸ ë°©ì‹ ì ìš©ìš©
            self.mean = alpha * self.mean + (1-alpha) * new_value
            self.std = np.sqrt(alpha * self.std**2 + (1-alpha) * (new_value - self.mean)**2)

        # í‘œì¤€í¸ì°¨ ê¸°ë°˜ ë™ì  ìŠ¤ì¼€ì¼ë§ íŒ©í„°
        scaling_factor = 1 / (1+self.std)
        
        return self.mean+ 1.5 * self.std * scaling_factor

    def compute_final_threshold(self, obj_id, new_value, frame_idx):
        """ì‹ ë¢° êµ¬ê°„ ë°©ì‹, ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸, variance adaptive, percentile ë°©ì‹ì„ ì¡°í•©í•˜ì—¬ ë™ì  ì„ê³„ê°’ ê²°ì •"""

        # âœ… ë™ì  íë¥¼ í™œìš©í•˜ì—¬ ìµœê·¼ depth ë³€í™”ëŸ‰ ì—…ë°ì´íŠ¸
        depth_diff_history = manager.update_depth(obj_id, new_value, frame_idx)

        if len(depth_diff_history) < 3:
            return new_value  # ë°ì´í„° ë¶€ì¡± ì‹œ í˜„ì¬ ëìŠ¤ê°’ ë°˜í™˜ (obj_depth â†’ new_value)

        # âœ… ê°œë³„ ì„ê³„ê°’ ê³„ì‚°
        std_dev = np.std([depth for _, depth in depth_diff_history])  # í‘œì¤€í¸ì°¨ ê³„ì‚°
        variance_threshold = compute_variance_adaptive_threshold(std_dev)
        percentile_threshold = compute_percentile_threshold([depth for _, depth in depth_diff_history])
        ci_threshold = self.compute_confidence_interval_threshold([depth for _, depth in depth_diff_history])
        bayesian_threshold = self.update_bayesian(new_value, [depth for _, depth in depth_diff_history])

        # âœ… Varianceì™€ Percentileì„ ë°”íƒ•ìœ¼ë¡œ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        variance_weight = 0.6 * np.clip(1 - std_dev, 0, 1)  # í‘œì¤€í¸ì°¨ ì‘ì„ìˆ˜ë¡ variance ë°˜ì˜
        percentile_weight = 1 - variance_weight  # ë‚˜ë¨¸ì§€ ê°€ì¤‘ì¹˜ëŠ” Percentileì— ë¶€ì—¬

        # âœ… CIì™€ Bayesian ë°©ì‹ì˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        ci_weight = 0.5 + (0.5 * np.clip(std_dev, 0, 1))  # í‘œì¤€í¸ì°¨ í´ìˆ˜ë¡ CI ì¤‘ìš”
        bayesian_weight = 1 - ci_weight

        # âœ… ì„ê³„ê°’ ìƒí•œ ì¡°ì • (ë³€ë™ì„±ì´ í¬ë©´ ì—¬ìœ ìˆê²Œ ì¡°ì •)
        max_threshold = new_value * (1.1 if std_dev > 0.2 else 1.0)  # obj_depth â†’ new_value

        # âœ… ìµœì¢… ì„ê³„ê°’ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì¡°í•©)
        final_threshold = (
        variance_weight * variance_threshold +
        percentile_weight * percentile_threshold + 
        ci_weight * ci_threshold +
        bayesian_weight * bayesian_threshold
    )

        # âœ… ìµœëŒ“ê°’ ì œí•œ ì ìš©
        final_threshold = min(final_threshold, max_threshold)

        #print(f"Variance Threshold: {variance_threshold:.4f}, Percentile Threshold: {percentile_threshold:.4f}, CI Threshold: {ci_threshold:.4f}, Bayesian Threshold: {bayesian_threshold:.4f}, Final Threshold: {final_threshold:.4f}")

        return final_threshold

adaptive_threshold = AdaptiveThreshold()

####################################################################################################################### Motion & Depth ################################################################################################################

class AdaptiveMotionDepthTracker:
    def __init__(
        self, 
        depth_threshold_ratio=0.05,      # ê¹Šì´ ë³€í™” ì„ê³„ê°’ ë¹„ìœ¨
        motion_threshold=0.05,           # ëª¨ì…˜ ë³€í™”ëŸ‰ ì„ê³„ê°’
        tracking_sensitivity=0.7,        # íŠ¸ë˜í‚¹ ë¯¼ê°ë„
        adaptation_rate=0.3,             # ê°€ì¤‘ì¹˜ ì ì‘ ì†ë„ (0~1)
        min_weight=0.2,                  # ìµœì†Œ ê°€ì¤‘ì¹˜ (0~1)
        max_weight=0.8                   # ìµœëŒ€ ê°€ì¤‘ì¹˜ (0~1)
    ):
        self.depth_history = {}          # ê¹Šì´ íˆìŠ¤í† ë¦¬: {obj_id: [(frame_idx, depth_value), ...]}
        self.motion_history = {}         # ëª¨ì…˜ íˆìŠ¤í† ë¦¬: {obj_id: [(frame_idx, motion_vector), ...]}
        self.prev_frame = None
        self.depth_threshold_ratio = depth_threshold_ratio
        self.motion_threshold = motion_threshold
        self.tracking_sensitivity = tracking_sensitivity
        self.adaptation_rate = adaptation_rate
        self.min_weight = min_weight
        self.max_weight = max_weight
        self.weight_history = {}         # ê°ì²´ë³„ ê°€ì¤‘ì¹˜ íˆìŠ¤í† ë¦¬: {obj_id: current_weight}
        self.depth_threshold_manager = DepthThresholdManager()
        self.adaptive_threshold = AdaptiveThreshold()
        
    def compute_optical_flow_motion(self, prev_frame, curr_frame, obj_mask):
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)
        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_RGB2GRAY)

        flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, 
                                        None, 0.5, 3, 15, 3, 5, 1.2, 0)

        flow_x = flow[:, :, 0]
        flow_y = flow[:, :, 1]

        # ê°ì²´ ë§ˆìŠ¤í¬ ì˜ì—­ í‰ê·  ëª¨ì…˜ ê³„ì‚°
        if obj_mask.sum() > 0:
            mean_x = np.mean(flow_x[obj_mask])
            mean_y = np.mean(flow_y[obj_mask])
        else:
            mean_x, mean_y = 0.0, 0.0

        return (mean_x, mean_y)


    def compute_motion_magnitude(self, motion_vector):
        """ëª¨ì…˜ ë²¡í„°ì˜ í¬ê¸° ê³„ì‚°"""
        return np.sqrt(motion_vector[0]**2 + motion_vector[1]**2)

    def update_object_history(self, obj_id, depth_value, motion_vector, frame_idx):
        """ê°ì²´ì˜ ê¹Šì´ì™€ ëª¨ì…˜ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        # ê¹Šì´ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ë° ì—…ë°ì´íŠ¸
        if obj_id not in self.depth_history:
            self.depth_history[obj_id] = []
        self.depth_history[obj_id].append((frame_idx, depth_value))
        self.depth_history[obj_id] = self.depth_history[obj_id][-10:]  # ìµœê·¼ 10ê°œë§Œ ìœ ì§€
        
        # ëª¨ì…˜ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ë° ì—…ë°ì´íŠ¸
        if obj_id not in self.motion_history:
            self.motion_history[obj_id] = []
        self.motion_history[obj_id].append((frame_idx, motion_vector))
        self.motion_history[obj_id] = self.motion_history[obj_id][-10:]  # ìµœê·¼ 10ê°œë§Œ ìœ ì§€
        
        # ê°€ì¤‘ì¹˜ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        if obj_id not in self.weight_history:
            self.weight_history[obj_id] = 0.5  # ì´ˆê¸° ê°€ì¤‘ì¹˜ëŠ” ê· ë“±í•˜ê²Œ ì„¤ì •

    def calculate_depth_change(self, obj_id):
        """ê¹Šì´ ë³€í™”ëŸ‰ ê³„ì‚°"""
        if len(self.depth_history[obj_id]) < 2:
            return 0.0
        
        depth_values = [d for _, d in self.depth_history[obj_id]]
        # ìµœê·¼ 5ê°œ í”„ë ˆì„ ë˜ëŠ” ëª¨ë“  í”„ë ˆì„ì˜ ê¹Šì´ ê°’ì„ ì‚¬ìš©í•˜ì—¬ ë³€í™”ëŸ‰ ê³„ì‚°
        recent_depth = depth_values[-min(5, len(depth_values)):]
        if len(recent_depth) < 2:
            return 0.0
            
        # í‘œì¤€í¸ì°¨ ê³„ì‚° (ë³€í™”ëŸ‰ì˜ ì§€í‘œ)
        depth_std = np.std(recent_depth)
        # ìµœëŒ€-ìµœì†Œ ë³€í™”ëŸ‰ ê³„ì‚°
        depth_range = np.max(recent_depth) - np.min(recent_depth)
        # í‰ê·  ëŒ€ë¹„ ë³€í™” ë¹„ìœ¨ ê³„ì‚°
        depth_mean = np.mean(recent_depth)
        depth_change_ratio = (depth_range / depth_mean) if depth_mean > 0 else 0
        
        # ë³€í™”ëŸ‰ ì ìˆ˜ ê³„ì‚° (í‘œì¤€í¸ì°¨ì™€ ë³€í™” ë¹„ìœ¨ì˜ ì¡°í•©)
        depth_change_score = depth_std * 0.5 + depth_change_ratio * 0.5
        
        return depth_change_score

    def calculate_motion_change(self, obj_id):
        """ëª¨ì…˜ ë³€í™”ëŸ‰ ê³„ì‚°"""
        if len(self.motion_history[obj_id]) < 2:
            return 0.0
        
        # ìµœê·¼ 5ê°œ í”„ë ˆì„ ë˜ëŠ” ëª¨ë“  í”„ë ˆì„ì˜ ëª¨ì…˜ ë²¡í„° ì‚¬ìš©
        recent_motion = self.motion_history[obj_id][-min(5, len(self.motion_history[obj_id])):]
        
        # ê° ëª¨ì…˜ ë²¡í„°ì˜ í¬ê¸° ê³„ì‚°
        motion_magnitudes = [self.compute_motion_magnitude(mv) for _, mv in recent_motion]
        
        if len(motion_magnitudes) < 2:
            return 0.0
            
        # ëª¨ì…˜ ë³€í™”ëŸ‰ ê³„ì‚° (í‘œì¤€í¸ì°¨ ì‚¬ìš©)
        motion_std = np.std(motion_magnitudes)
        # ìµœëŒ€ ëª¨ì…˜ í¬ê¸° ê³„ì‚°
        max_motion = np.max(motion_magnitudes)
        
        # ë³€í™”ëŸ‰ ì ìˆ˜ ê³„ì‚° (í‘œì¤€í¸ì°¨ì™€ ìµœëŒ€ ëª¨ì…˜ì˜ ì¡°í•©)
        motion_change_score = motion_std * 0.3 + max_motion * 0.7
        
        return motion_change_score

    def update_adaptive_weight(self, obj_id):
        """ëìŠ¤ì™€ ëª¨ì…˜ ë³€í™”ëŸ‰ì— ê¸°ë°˜í•œ ì ì‘í˜• ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        depth_change = self.calculate_depth_change(obj_id)
        motion_change = self.calculate_motion_change(obj_id)
        
        # ë³€í™”ëŸ‰ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ê°€ì¤‘ì¹˜ ìœ ì§€
        if depth_change < 0.01 and motion_change < 0.01:
            return self.weight_history[obj_id]
        
        # ëìŠ¤ì™€ ëª¨ì…˜ ë³€í™”ëŸ‰ì˜ ìƒëŒ€ì  ë¹„ìœ¨ ê³„ì‚°
        total_change = depth_change + motion_change
        
        if total_change > 0:
            # ëìŠ¤ ë³€í™” ë¹„ìœ¨ ê³„ì‚° (a ê°’)
            depth_ratio = depth_change / total_change
            # í˜„ì¬ ê°€ì¤‘ì¹˜ì™€ ìƒˆë¡œìš´ ë¹„ìœ¨ì„ í˜¼í•©í•˜ì—¬ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            new_weight = (1 - self.adaptation_rate) * self.weight_history[obj_id] + self.adaptation_rate * depth_ratio
            # ê°€ì¤‘ì¹˜ ë²”ìœ„ ì œí•œ
            new_weight = max(self.min_weight, min(self.max_weight, new_weight))
            self.weight_history[obj_id] = new_weight
            
            print(f"Object {obj_id} | Depth Change: {depth_change:.4f} | Motion Change: {motion_change:.4f} | Weight: {new_weight:.4f}")
        
        return self.weight_history[obj_id]

    def should_track(self, obj_id, depth_value, motion_vector, frame_idx):
        """ê°ì²´ ì¶”ì  ì—¬ë¶€ ê²°ì •"""
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.update_object_history(obj_id, depth_value, motion_vector, frame_idx)
        
        # ê°ì²´ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (a ê°’)
        weight = self.update_adaptive_weight(obj_id)
        
        # ëìŠ¤ ê¸°ë°˜ ìŠ¤ë ˆìŠ¤í™€ë“œ ê³„ì‚°
        depth_queue = self.depth_threshold_manager.update_depth(obj_id, depth_value, frame_idx)
        depth_threshold = self.adaptive_threshold.compute_final_threshold(obj_id, depth_value, frame_idx)
        
        # ëª¨ì…˜ í¬ê¸° ê³„ì‚°
        motion_magnitude = self.compute_motion_magnitude(motion_vector)
        
        # ëìŠ¤ ë³€í™”ëŸ‰ ê³„ì‚°
        depth_values = [d for _, d in depth_queue]
        last_depth = depth_values[-1] if depth_values else depth_value
        depth_diff = abs(depth_value - last_depth)
        
        # ëìŠ¤ ê¸°ë°˜ ê²°ì • (ëìŠ¤ ë³€í™”ëŸ‰ì´ ì„ê³„ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ ì¶”ì  ì¤‘ë‹¨)
        depth_decision = depth_diff < depth_threshold
        
        # ëª¨ì…˜ ê¸°ë°˜ ê²°ì • (ëª¨ì…˜ í¬ê¸°ê°€ ì„ê³„ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ ì¶”ì  ì¤‘ë‹¨)
        motion_decision = motion_magnitude < self.motion_threshold
        
        # ê°€ì¤‘ì¹˜ ì ìš©ëœ ìµœì¢… ê²°ì • (ê°€ì¤‘í•© ê³„ì‚°)
        # ê°€ì¤‘ì¹˜(weight)ëŠ” ëìŠ¤ì˜ ì˜í–¥ë ¥ì„ ì˜ë¯¸í•˜ê³ , (1-weight)ëŠ” ëª¨ì…˜ì˜ ì˜í–¥ë ¥ì„ ì˜ë¯¸í•¨
        # ë‘ ê²°ì •ì„ ê°€ì¤‘í•©ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ìµœì¢… ê²°ì •
        weighted_decision = weight * depth_decision + (1 - weight) * motion_decision
        
        # ìµœì¢… ê²°ì • ê°’ì´ 0.5 ì´ìƒì´ë©´ ì¶”ì  ì¤‘ë‹¨ (TrueëŠ” ì¶”ì  ì¤‘ë‹¨, FalseëŠ” ì¶”ì  ê³„ì†)
        track_decision = weighted_decision >= 0.7
        
        print(f"Object {obj_id} | Depth Decision: {depth_decision} | Motion Decision: {motion_decision} | Weighted: {weighted_decision:.4f} | Track: {not track_decision}")
        
        return not track_decision  # ì¶”ì  ì—¬ë¶€ ë°˜í™˜ (True: ì¶”ì , False: ì¶”ì  ì¤‘ë‹¨)

    def should_skip_tracking(self, obj_id, depth_value, motion_vector, frame_idx):
        """ì¶”ì  ê±´ë„ˆë›°ê¸° ì—¬ë¶€ ê²°ì • (should_trackê³¼ ë°˜ëŒ€ ì˜ë¯¸)"""
        return not self.should_track(obj_id, depth_value, motion_vector, frame_idx)

    def adjust_tracking_parameters(self, depth_threshold_ratio=None, motion_threshold=None, 
                                 tracking_sensitivity=None, adaptation_rate=None, 
                                 min_weight=None, max_weight=None):
        """ì¶”ì  íŒŒë¼ë¯¸í„° ì¡°ì •"""
        if depth_threshold_ratio is not None:
            self.depth_threshold_ratio = depth_threshold_ratio
        if motion_threshold is not None:
            self.motion_threshold = motion_threshold
        if tracking_sensitivity is not None:
            self.tracking_sensitivity = tracking_sensitivity
        if adaptation_rate is not None:
            self.adaptation_rate = adaptation_rate
        if min_weight is not None:
            self.min_weight = min_weight
        if max_weight is not None:
            self.max_weight = max_weight
    def should_pause_tracking(self, obj_id, depth_value, motion_vector, frame_idx):
        return not self.should_track(obj_id, depth_value, motion_vector, frame_idx)

    def should_resume_tracking(self, obj_id, depth_value, motion_vector, frame_idx):
        return self.should_track(obj_id, depth_value, motion_vector, frame_idx)



# ê¹Šì´ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ
pipe = pipeline(task="depth-estimation", model="depth-anything/Depth-Anything-V2-Base-hf", device=0)

def compute_dynamic_alpha(depth_diff_history, base_k=0.1):
    """ê¹Šì´ ë³€í™”ëŸ‰(í‘œì¤€í¸ì°¨)ì— ë”°ë¼ Î± ê°’ì„ ë™ì ìœ¼ë¡œ ì¡°ì •"""
    depth_diff_history = np.array(depth_diff_history)
    depth_diff_history = depth_diff_history[~np.isnan(depth_diff_history)]  # NaN ì œê±°

    if len(depth_diff_history) < 10:
        return 0.9  # ìµœì†Œ ìƒ˜í”Œ ê°œìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ê°’ ìœ ì§€

    std_depth = np.std(depth_diff_history)
    
    # k ê°’ì„ ë°ì´í„°ì˜ ì¤‘ì•™ê°’(median) ê¸°ë°˜ìœ¼ë¡œ ë™ì  ì¡°ì •
    median_value = np.median(depth_diff_history)
    k = base_k * median_value if median_value > 0 else base_k

    # Î± ê³„ì‚° (ìµœëŒ€ê°’ 0.95ë¡œ ì œí•œ)
    alpha = 0.95 - (std_depth / (std_depth + k))

    return np.clip(alpha, 0.7, 0.95)  # ê°’ ë²”ìœ„ ì œí•œ


def compute_dynamic_confidence(depth_diff_history):
    """ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ ë¹„ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ confidence ê°’ì„ ìë™ ì¡°ì •"""
    depth_diff_history = np.array(depth_diff_history)
    depth_diff_history = depth_diff_history[~np.isnan(depth_diff_history)]  # NaN ì œê±°

    if len(depth_diff_history) < 5:
        return 0.95  # ê¸°ë³¸ê°’ ë°˜í™˜

    Q1, Q3 = np.percentile(depth_diff_history, [25, 75])
    IQR = Q3 - Q1
    outliers_iqr = np.sum((depth_diff_history < (Q1 - 1.5 * IQR)) | (depth_diff_history > (Q3 + 1.5 * IQR)))

     # MAD ë°©ì‹ ì´ìƒì¹˜ ê²€ì¶œ
    median = np.median(depth_diff_history)
    MAD = np.median(np.abs(depth_diff_history - median)) * 1.4826
    outliers_mad = np.sum((depth_diff_history < (median - 3 * MAD)) | (depth_diff_history > (median + 3 * MAD)))

    # ì´ìƒì¹˜ ê°œìˆ˜ í‰ê· í™”
    outliers = (outliers_iqr + outliers_mad) / 2

    # ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚°
    outlier_ratio = outliers / len(depth_diff_history)

    # ì§€ìˆ˜ ê°ì‡„ ë°©ì‹ ì ìš© (Î»=3)
    lambda_factor = 3
    confidence = 0.95 * np.exp(-lambda_factor * outlier_ratio)

    return np.clip(confidence, 0.9, 0.95)  # ì‹ ë¢°êµ¬ê°„ ë²”ìœ„ ì œí•œ

def compute_percentile_threshold(depth_diff_history, k=1.5):
    """í¼ì„¼íƒ€ì¼ ê¸°ë°˜ ì„ê³„ê°’ ì„¤ì • (Percentile-Based Thresholding)"""
    depth_diff_history = np.array(depth_diff_history)
    depth_diff_history = depth_diff_history[~np.isnan(depth_diff_history)]

    if len(depth_diff_history) < 5:
        return np.mean(depth_diff_history) if len(depth_diff_history) > 0 else 0

    Q1 = np.percentile(depth_diff_history, 25)
    Q3 = np.percentile(depth_diff_history, 75)
    IQR = Q3 - Q1

    return Q3 + k * IQR

def compute_variance_adaptive_threshold(depth_diff_history, k_base=1.5):
    """ë³€ë™ì„± ê¸°ë°˜ ì„ê³„ê°’ ì„¤ì • (Variance-Adaptive Thresholding)"""
    depth_diff_history = np.array(depth_diff_history)
    depth_diff_history = depth_diff_history[~np.isnan(depth_diff_history)]

    if len(depth_diff_history) < 5:
        return np.mean(depth_diff_history) if len(depth_diff_history) > 0 else 0

    mean_diff = np.mean(depth_diff_history)
    std_diff = np.std(depth_diff_history)

    # ë³€ë™ì„± ê¸°ë°˜ìœ¼ë¡œ k ê°’ì„ ì¡°ì • (stdê°€ í´ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ì¦ê°€)
    k_dynamic = k_base * (1 + (std_diff / (mean_diff + 1e-6)))

    return mean_diff + k_dynamic * std_diff


class AOTTracker(object):
    def __init__(self, cfg, gpu_id):
        self.with_crop = False
        self.EXPAND_SCALE = None
        self.small_ratio = 12
        self.mid_ratio = 100
        self.large_ratio = 0.5
        self.AOT_INPUT_SIZE = (465, 465)
        self.cnt = 2
        self.gpu_id = gpu_id
        self.model = build_vos_model(cfg.MODEL_VOS, cfg).cuda(gpu_id)
        self.model.cuda(gpu_id)
        self.model.eval()
        print('cfg.TEST_CKPT_PATH = ', cfg.TEST_CKPT_PATH)
        self.model, _ = load_network(self.model, cfg.TEST_CKPT_PATH, gpu_id)
        self.aug_nums = len(cfg.TEST_MULTISCALE)
        if cfg.TEST_FLIP:
            self.aug_nums *= 2
        self.engine = []
        for aug_idx in range(self.aug_nums):
            self.engine.append(build_engine(cfg.MODEL_ENGINE,
                                            phase='eval',
                                            aot_model=self.model,
                                            gpu_id=gpu_id,
                                            short_term_mem_skip=cfg.TEST_SHORT_TERM_MEM_SKIP,
                                            long_term_mem_gap=cfg.TEST_LONG_TERM_MEM_GAP,
                                            ))
            self.engine[-1].eval()
        self.transform = transforms.Compose([
            tr.MultiRestrictSize_(cfg.TEST_MAX_SHORT_EDGE,
                                  cfg.TEST_MAX_LONG_EDGE, cfg.TEST_FLIP, cfg.TEST_INPLACE_FLIP,
                                  cfg.TEST_MULTISCALE, cfg.MODEL_ALIGN_CORNERS),
            tr.MultiToTensor()
        ])
    
    def add_first_frame(self, frame, mask):
        sample = {'current_img': frame, 'current_label': mask}
        sample = self.transform(sample)
        frame = sample[0]['current_img'].unsqueeze(0).float().cuda(self.gpu_id)
        mask = sample[0]['current_label'].unsqueeze(0).float().cuda(self.gpu_id)
        mask = F.interpolate(mask, size=frame.size()[2:], mode="nearest")
        self.engine[0].add_reference_frame(frame, mask, frame_step=0, obj_nums=int(mask.max()))

    def track(self, image):
        height = image.shape[0]
        width = image.shape[1]
        sample = {'current_img': image}
        sample['meta'] = {
            'height': height,
            'width': width,
            'flip': False
        }
        sample = self.transform(sample)

        if self.aug_nums > 1:
            torch.cuda.empty_cache()
        all_preds = []
        for aug_idx in range(self.aug_nums):
            output_height = sample[aug_idx]['meta']['height']
            output_width = sample[aug_idx]['meta']['width']
            image = sample[aug_idx]['current_img'].unsqueeze(0).float().cuda(self.gpu_id, non_blocking=True)
            image = image.cuda(self.gpu_id, non_blocking=True)
            self.engine[aug_idx].match_propogate_one_frame(image) #network->engine->aot_engine.py->aotengine
            is_flipped = sample[aug_idx]['meta']['flip']
            pred_logit = self.engine[aug_idx].decode_current_logits((output_height, output_width))
            if is_flipped:
                pred_logit = flip_tensor(pred_logit, 3)
            pred_prob = torch.softmax(pred_logit, dim=1)
            all_preds.append(pred_prob)
            cat_all_preds = torch.cat(all_preds, dim=0)
            pred_prob = torch.mean(cat_all_preds, dim=0, keepdim=True)
            pred_label = torch.argmax(pred_prob, dim=1, keepdim=True).float()
            _pred_label = F.interpolate(pred_label,
                                        size=self.engine[aug_idx].input_size_2d,
                                        mode="nearest")
            self.engine[aug_idx].update_memory(_pred_label)
            mask = pred_label.detach().cpu().numpy()[0][0].astype(np.uint8)
        conf = 0

        return mask, conf

# HQTrack ë‚´ë¶€ í†µí•©
class HQTrack(object):
    def __init__(self, cfg, config, local_track=False, sam_refine=False, sam_refine_iou=0):
        self.aot_tracker = AOTTracker(cfg, config['gpu_id'])
        self.local_track = local_track
        self.sam_refine = sam_refine
        self.sam_refine_iou = sam_refine_iou
        self.frame_idx = 0
        self.motion_tracker = AdaptiveMotionDepthTracker()  # ëª¨ì…˜ ê¸°ë°˜ íŠ¸ë˜ì»¤ í†µí•©

    def initialize(self, image, mask):
        self.tracker = self.aot_tracker
        self.tracker.add_first_frame(image, mask)
        self.mask_size = mask.shape
        self.obj_num = int(mask.max())

    def track(self, image):
        depth_result = pipe(Image.fromarray(image))
        depth_map = np.array(depth_result["depth"])
        all_masks = []
        last_masks = getattr(self, 'last_masks', [None] * self.obj_num)
        confidence = 0

        if not hasattr(self, 'tracking_status'):
            self.tracking_status = {obj_id: True for obj_id in range(self.obj_num)}

        for obj_id in range(self.obj_num):
            mask_bool = last_masks[obj_id] if last_masks[obj_id] is not None else np.zeros_like(depth_map, dtype=bool)
            obj_depth = np.mean(depth_map[mask_bool]) if np.any(mask_bool) else 0.0

            # Optical Flow ê³„ì‚°
            if self.frame_idx > 0 and getattr(self, 'prev_frame', None) is not None:
                motion_vector = self.motion_tracker.compute_optical_flow_motion(self.prev_frame, image, mask_bool)
            else:
                motion_vector = (0.0, 0.0)

            if not self.tracking_status[obj_id]:
                # ì¬ê°œ ì—¬ë¶€ í™•ì¸
                resume = self.motion_tracker.should_resume_tracking(obj_id, obj_depth, motion_vector, self.frame_idx)
                if resume:
                    print(f"[RESUME] Resuming tracking for object {obj_id}")
                    self.tracking_status[obj_id] = True
                else:
                    print(f"[SKIP] Object {obj_id} still below threshold")
                if last_masks[obj_id] is not None:
                    obj_mask = np.zeros_like(last_masks[obj_id], dtype=np.uint8)
                    obj_mask[last_masks[obj_id]] = obj_id + 1
                    all_masks.append(obj_mask)
                continue

            should_stop = self.motion_tracker.should_pause_tracking(obj_id, obj_depth, motion_vector, self.frame_idx)
            if should_stop:
                print(f"[PAUSE] Temporarily stopping tracking for object {obj_id}")
                self.tracking_status[obj_id] = False
                if last_masks[obj_id] is not None:
                    obj_mask = np.zeros_like(last_masks[obj_id], dtype=np.uint8)
                    obj_mask[last_masks[obj_id]] = obj_id + 1
                    all_masks.append(obj_mask)
                continue

            # AOT ì¶”ì  ì‹¤í–‰
            m, curr_confidence = self.tracker.track(image)
            confidence = max(confidence, curr_confidence)
            obj_mask = (m == obj_id + 1)
            last_masks[obj_id] = obj_mask

            # ì—…ë°ì´íŠ¸
            obj_depth = np.mean(depth_map[obj_mask]) if np.any(obj_mask) else 0.0
            self.motion_tracker.update_object_history(obj_id, obj_depth, motion_vector, self.frame_idx)
            all_masks.append(m)

        self.last_masks = last_masks
        self.prev_frame = image
        self.frame_idx += 1

        if not all_masks:
            dummy_mask = np.zeros_like(depth_map, dtype=np.uint8)
            return dummy_mask, 0

        return np.max(all_masks, axis=0), confidence

def OnMouse_box(event,x,y,flags,param):
    global x0, y0, img4show, img
    if event == cv2.EVENT_LBUTTONDOWN:
        x0,y0 =x,y
    elif event == cv2.EVENT_MOUSEMOVE and flags == cv2.EVENT_FLAG_LBUTTON:
        x_temp, y_temp = x, y
        img4show=img.copy()
        cv2.rectangle(img4show, (x0, y0), (x_temp, y_temp), (255, 255, 0), 2)
    elif event == cv2.EVENT_LBUTTONUP:
        x1, y1 = x, y
        cv2.rectangle(img4show, (x0, y0), (x, y), (255, 255, 0), 2)
        img=img4show
        point_box_prompts.append([x0, y0, x1, y1])

def OnMouse_point(event,x,y,flags,param):
    global x0, y0, img4show, img
    if event == cv2.EVENT_LBUTTONDOWN:
        x0,y0 =x,y
        point_box_prompts.append([x0,y0])
    elif event == cv2.EVENT_LBUTTONUP:
        cv2.circle(img4show, (x0, y0), 4, (0, 255, 0), 6)
        img=img4show

if __name__ == "__main__":
    print("Initializing SAM and HQTrack...")
    # SAM ì„¤ì •
    model_type = 'vit_h'
    sam_checkpoint = os.path.join(base_path, '..', 'segment_anything_hq/pretrained_model/sam_hq_vit_h.pth')
    output_mode = "binary_mask"
    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
    sam.to(device=torch.device('cuda'))
    mask_generator = SamAutomaticMaskGenerator(sam, output_mode=output_mode)
    mask_prompt = SamPredictor(sam)

    # ì„¤ì •
    SAM_prompt = 'Box'
    tracker_type = 'HQTrack'
    sam_refine = True
    sam_refine_iou = 0.1
    epoch_num = 42000
    config = {
        'exp_name': 'default',
        'model': 'internT_msdeaotl_v2',
        'pretrain_model_path': f'result/default_InternT_MSDeAOTL_V2/YTB_DAV_VIP/ckpt/save_step_{epoch_num}.pth',
        'gpu_id': 0,
    }

    if tracker_type == 'HQTrack':
        engine_config = importlib.import_module('configs.' + 'ytb_vip_dav_deaot_internT')
        cfg = engine_config.EngineConfig(config['exp_name'], config['model'])
        cfg.TEST_CKPT_PATH = os.path.join(AOT_PATH, config['pretrain_model_path'])

    palette_template = Image.open(os.path.join(os.path.dirname(__file__), '..', 'my_tools/mask_palette.png')).getpalette()
    tracker = HQTrack(cfg, config, True, sam_refine, sam_refine_iou)

    save_dir = os.path.join(base_path, 'output')
    os.makedirs(save_dir, exist_ok=True)

    for idx, img_file in enumerate(img_files):
        print(f"\n[Frame {idx}] Processing: {img_file}")
        img = cv2.imread(img_file, cv2.IMREAD_UNCHANGED)
        if img is None:
            print(f"Error loading image: {img_file}")
            continue
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_ori = img.copy()

        if idx == 0:
            # ROI ìˆ˜ë™ ì„ íƒ
            img4show = img.copy()
            while True:
                cv2.namedWindow("Select ROI", cv2.WINDOW_NORMAL)
                cv2.imshow("Select ROI", cv2.cvtColor(img4show, cv2.COLOR_RGB2BGR))
                if SAM_prompt == 'Box':
                    OnMouse = OnMouse_box
                elif SAM_prompt == 'Point':
                    OnMouse = OnMouse_point
                cv2.setMouseCallback('Select ROI', OnMouse)
                if cv2.waitKey(1) == ord('r'):
                    break

            # ë§ˆìŠ¤í¬ ìƒì„± ë° íŠ¸ë˜ì»¤ ì´ˆê¸°í™”
            start_time = time.time()
            mask_2 = np.zeros_like(img[:, :, 0])
            masks_ls = [mask_2]
            for obj_idx, prompt in enumerate(point_box_prompts):
                mask_prompt.set_image(img_ori)
                if SAM_prompt == 'Box':
                    masks_, iou_predictions, _ = mask_prompt.predict(box=np.array(prompt).astype(float))
                elif SAM_prompt == 'Point':
                    masks_, iou_predictions, _ = mask_prompt.predict(point_labels=np.asarray([1]), point_coords=np.asarray([prompt]))
                select_index = np.argmax(iou_predictions)
                init_mask = masks_[select_index].astype(np.uint8)
                masks_ls.append(init_mask)
                mask_2 += init_mask * (obj_idx + 1)

            masks_stack = np.stack(masks_ls)
            masks_combined = np.where(masks_stack.sum(0) > 1, np.argmax(masks_stack, axis=0), mask_2)
            init_masks = [(masks_combined == (i + 1)).astype(np.uint8) for i in range(len(masks_ls) - 1)]
            tracker.initialize(img_ori, masks_combined)
            obj_num = len(init_masks)
            print(f"Initialized {obj_num} objects.")

            # âœ… ì²« í”„ë ˆì„ë„ ì‹œê°í™” ë° ì €ì¥
            img_vis = cv2.cvtColor(img_ori.astype(np.float32), cv2.COLOR_RGB2BGR)
            for i, mask in enumerate(init_masks):
                img_vis[:, :, 1] += 127.0 * mask
                img_vis[:, :, 2] += 127.0 * mask
                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
                img_vis = cv2.drawContours(img_vis, contours, -1, cur_colors[i], 2)
            img_vis = np.clip(img_vis, 0, 255).astype(np.uint8)
            cv2.putText(img_vis, 'Init', (35, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 5)
            cv2.imshow('demo', img_vis)
            cv2.waitKey(1)  # <- í˜¹ì‹œë¼ë„ êº¼ì§ˆê¹Œë´ ì¶”ê°€
            save_path = os.path.join(save_dir, os.path.basename(img_file))
            cv2.imwrite(save_path, img_vis)

        else:
            # ì¶”ì 
            m, confidence = tracker.track(img_ori)
            if m is None:
                print(f"[Warning] No mask returned for frame {idx}")
                continue
            print(f"Confidence: {confidence:.4f}")
            pred_masks = [(m == (i + 1)).astype(np.uint8) for i in range(obj_num)]

            # ì‹œê°í™” ë° ì €ì¥
            img_vis = cv2.cvtColor(img_ori.astype(np.float32), cv2.COLOR_RGB2BGR)
            for i, mask in enumerate(pred_masks):
                img_vis[:, :, 1] += 127.0 * mask
                img_vis[:, :, 2] += 127.0 * mask
                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
                img_vis = cv2.drawContours(img_vis, contours, -1, cur_colors[i], 2)
            img_vis = np.clip(img_vis, 0, 255).astype(np.uint8)

            save_path = os.path.join(save_dir, os.path.basename(img_file))
            cv2.imwrite(save_path, img_vis)
            result = cv2.imwrite(save_path, img_vis)
            print(f"[Save Result] {result}, Path: {save_path}")
end_time = time.time()
elapsed_time = end_time - start_time  # ê²½ê³¼ ì‹œê°„ ê³„ì‚°
print(f"Tracking completed in {elapsed_time:.2f} seconds.")